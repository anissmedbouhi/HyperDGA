{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1151c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.dataset import binary_tree\n",
    "from lib import functions\n",
    "from lib.dataset.binary_tree import *\n",
    "import time\n",
    "\n",
    "import itertools\n",
    "import numpy\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import hdbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib import pyplot as plot\n",
    "\n",
    "\n",
    "from math import inf\n",
    "\n",
    "\n",
    "# --- Misc. geometry code -----------------------------------------------------\n",
    "\n",
    "'''\n",
    "Pick N points uniformly from the unit disc\n",
    "This sampling algorithm does not use rejection sampling.\n",
    "'''\n",
    "def disc_uniform_pick(N):\n",
    "\tangle = (2 * numpy.pi) * numpy.random.random(N)\n",
    "\tout = numpy.stack([numpy.cos(angle), numpy.sin(angle)], axis = 1)\n",
    "\tout *= numpy.sqrt(numpy.random.random(N))[:,None]\n",
    "\treturn out\n",
    "\n",
    "\n",
    "\n",
    "def norm2(X):\n",
    "\treturn numpy.sqrt(numpy.sum(X ** 2))\n",
    "\n",
    "\n",
    "\n",
    "def normalized(X):\n",
    "\treturn X / norm2(X)\n",
    "\n",
    "\n",
    "\n",
    "# --- Delaunay triangulation --------------------------------------------------\n",
    "\n",
    "def get_triangle_normal(A, B, C):\n",
    "\treturn normalized(numpy.cross(A, B) + numpy.cross(B, C) + numpy.cross(C, A))\n",
    "\n",
    "\n",
    "\n",
    "def get_power_circumcenter(A, B, C):\n",
    "\tN = get_triangle_normal(A, B, C)\n",
    "\treturn (-.5 / N[2]) * N[:2]\n",
    "\n",
    "\n",
    "\n",
    "def is_ccw_triangle(A, B, C):\n",
    "\tM = numpy.concatenate([numpy.stack([A, B, C]), numpy.ones((3, 1))], axis = 1)\n",
    "\treturn numpy.linalg.det(M) > 0\n",
    "\n",
    "\n",
    "\n",
    "def get_power_triangulation(S, R):\n",
    "\t# Compute the lifted weighted points\n",
    "\tS_norm = numpy.sum(S ** 2, axis = 1) - R #instead of ...-R**2, because error in the paper where the formula gives R**2 and not R \n",
    "\tS_lifted = numpy.concatenate([S, S_norm[:,None]], axis = 1)\n",
    "\n",
    "\t# Special case for 3 points\n",
    "\tif S.shape[0] == 3:\n",
    "\t\tif is_ccw_triangle(S[0], S[1], S[2]):\n",
    "\t\t\treturn [[0, 1, 2]], numpy.array([get_power_circumcenter(*S_lifted)])\n",
    "\t\telse:\n",
    "\t\t\treturn [[0, 2, 1]], numpy.array([get_power_circumcenter(*S_lifted)])\n",
    "\n",
    "\t# Compute the convex hull of the lifted weighted points\n",
    "\thull = ConvexHull(S_lifted)\n",
    "\t\n",
    "\t# Extract the Delaunay triangulation from the lower hull\n",
    "\ttri_list = tuple([a, b, c] if is_ccw_triangle(S[a], S[b], S[c]) else [a, c, b]  for (a, b, c), eq in zip(hull.simplices, hull.equations) if eq[2] <= 0)\n",
    "\t\n",
    "\t# Compute the Voronoi points\n",
    "\tV = numpy.array([get_power_circumcenter(*S_lifted[tri]) for tri in tri_list])\n",
    "\n",
    "\t# Job done\n",
    "\treturn tri_list, V\n",
    "\n",
    "\n",
    "\n",
    "# --- Compute Voronoi cells ---------------------------------------------------\n",
    "\n",
    "'''\n",
    "Compute the segments and half-lines that delimits each Voronoi cell\n",
    "  * The segments are oriented so that they are in CCW order\n",
    "  * Each cell is a list of (i, j), (A, U, tmin, tmax) where\n",
    "     * i, j are the indices of two ends of the segment. Segments end points are\n",
    "       the circumcenters. If i or j is set to None, then it's an infinite end\n",
    "     * A is the origin of the segment\n",
    "     * U is the direction of the segment, as a unit vector\n",
    "     * tmin is the parameter for the left end of the segment. Can be -1, for minus infinity\n",
    "     * tmax is the parameter for the right end of the segment. Can be -1, for infinity\n",
    "     * Therefore, the endpoints are [A + tmin * U, A + tmax * U]\n",
    "'''\n",
    "def get_voronoi_cells(S, V, tri_list):\n",
    "    # Keep track of which circles are included in the triangulation\n",
    "    vertices_set = frozenset(itertools.chain(*tri_list))\n",
    "    #print('vertices_set:', vertices_set)\n",
    "\n",
    "    # Keep track of which edge separate which triangles\n",
    "    edge_map = { }\n",
    "    for i, tri in enumerate(tri_list):\n",
    "        for edge in itertools.combinations(tri, 2):\n",
    "            edge = tuple(sorted(edge))\n",
    "            if edge in edge_map:\n",
    "                edge_map[edge].append(i)\n",
    "            else:\n",
    "                edge_map[edge] = [i]\n",
    "    #print('edge_map:', edge_map)\n",
    "\n",
    "    # For each triangle\n",
    "    voronoi_cell_map = { i : [] for i in vertices_set }\n",
    "    #print('voronoi_cell_map before big loop:', voronoi_cell_map)\n",
    "\n",
    "    for i, (a, b, c) in enumerate(tri_list):\n",
    "        # For each edge of the triangle\n",
    "        #print('i:', i)\n",
    "        #print('(a, b, c):', (a, b, c))\n",
    "        for u, v, w in ((a, b, c), (b, c, a), (c, a, b)):\n",
    "        # Finite Voronoi edge\n",
    "            #print('u:', u) \n",
    "            #print('v:', v) \n",
    "            #print('w:', w)\n",
    "            edge = tuple(sorted((u, v)))\n",
    "            #print('edge:', edge)\n",
    "            if len(edge_map[edge]) == 2:\n",
    "                j, k = edge_map[edge]\n",
    "                if k == i:\n",
    "                    j, k = k, j\n",
    "                \n",
    "                # Compute the segment parameters\n",
    "                U = V[k] - V[j]\n",
    "                U_norm = norm2(U)\n",
    "\n",
    "                # Add the segment\n",
    "                voronoi_cell_map[u].append(((j, k), (V[j], U / U_norm, 0, U_norm)))\n",
    "                #print('voronoi_cell_map[u]:', voronoi_cell_map[u])\n",
    "            else: \n",
    "            # Infinite Voronoi edge\n",
    "                # Compute the segment parameters\n",
    "                A, B, C, D = S[u], S[v], S[w], V[i]\n",
    "                U = normalized(B - A)\n",
    "                I = A + numpy.dot(D - A, U) * U\n",
    "                W = normalized(I - D)\n",
    "                if numpy.dot(W, I - C) < 0:\n",
    "                    W = -W\n",
    "            \n",
    "                # Add the segment\n",
    "                voronoi_cell_map[u].append(((edge_map[edge][0], -1), (D,  W, 0, None)))\n",
    "                voronoi_cell_map[v].append(((-1, edge_map[edge][0]), (D, -W, None, 0)))\n",
    "\n",
    "    #print('voronoi_cell_map after big loop:', voronoi_cell_map)\n",
    "    \n",
    "    # Order the segments\n",
    "    def order_segment_list(segment_list):\n",
    "        # Pick the first element\n",
    "        first = min((seg[0][0], i) for i, seg in enumerate(segment_list))[1]\n",
    "\n",
    "        # In-place ordering\n",
    "        segment_list[0], segment_list[first] = segment_list[first], segment_list[0]\n",
    "        for i in range(len(segment_list) - 1):\n",
    "            for j in range(i + 1, len(segment_list)):\n",
    "                if segment_list[i][0][1] == segment_list[j][0][0]:\n",
    "                    segment_list[i+1], segment_list[j] = segment_list[j], segment_list[i+1]\n",
    "                    break\n",
    "\n",
    "        # Job done\n",
    "        return segment_list\n",
    "\n",
    "    # Job done\n",
    "    return { i : order_segment_list(segment_list) for i, segment_list in voronoi_cell_map.items() }\n",
    "\n",
    "\n",
    "def display(S, R, tri_list, voronoi_cell_map, true_edges=None, margen=0, points=None, title='Voronoi diagram and Delaunay complex'):\n",
    "    # Setup\n",
    "    #fig, ax = plot.subplots()\n",
    "    fig, ax = plot.subplots(figsize = (50, 50))\n",
    "    plot.axis('equal')\n",
    "    plot.axis('off')\n",
    "    #plot.scatter(S[:, 0], S[:, 1], s=20, c='black')\n",
    "    #plot.scatter(points[:, 0], points[:, 1], s=100, c=L, cmap='Spectral')\n",
    "    plot.scatter(points[:, 0], points[:, 1], s=100)\n",
    "\n",
    "\n",
    "#     # Set min/max display size, as Matplotlib does it wrong\n",
    "#     min_corner = numpy.amin(S, axis = 0) - numpy.max(R)\n",
    "#     max_corner = numpy.amax(S, axis = 0) + numpy.max(R)\n",
    "#     plot.xlim((min_corner[0], max_corner[0]))\n",
    "#     plot.ylim((min_corner[1], max_corner[1]))\n",
    "\n",
    "    # Set min/max display size, as Matplotlib does it wrong\n",
    "    min_corner_x = numpy.amin(S, axis = 0)-margen #- numpy.max(R)\n",
    "    max_corner_x = numpy.amax(S, axis = 0)+margen #+ numpy.max(R)\n",
    "    min_corner_y = numpy.amin(S, axis = 1)-margen #- numpy.max(R)\n",
    "    max_corner_y = numpy.amax(S, axis = 1)+margen #+ numpy.max(R)\n",
    "    plot.xlim((min_corner_x[0], max_corner_x[0]))\n",
    "    plot.ylim((min_corner_y[1], max_corner_y[1]))\n",
    "\n",
    "    # Plot the samples\n",
    "    for Si, Ri in zip(S, R):             #fill = True\n",
    "        ax.add_artist(plot.Circle(Si, Ri, fill = False, alpha = .1, lw = 0., color = '#8080f0', zorder = 1))\n",
    "\n",
    "    # Plot the power triangulation\n",
    "    edge_set = frozenset(tuple(sorted(edge)) for tri in tri_list for edge in itertools.combinations(tri, 2))\n",
    "    line_list = LineCollection([(S[i], S[j]) for i, j in edge_set], lw = 1., colors = 'green')\n",
    "    line_list.set_zorder(0)\n",
    "    ax.add_collection(line_list)\n",
    "\n",
    "    # Plot the Voronoi cells\n",
    "    edge_map = { }\n",
    "    for segment_list in voronoi_cell_map.values():\n",
    "        for edge, (A, U, tmin, tmax) in segment_list:\n",
    "            edge = tuple(sorted(edge))\n",
    "            if edge not in edge_map:\n",
    "                if tmax is None:\n",
    "                    tmax = 10\n",
    "                if tmin is None:\n",
    "                    tmin = -10\n",
    "\n",
    "                edge_map[edge] = (A + tmin * U, A + tmax * U)\n",
    "\n",
    "    line_list = LineCollection(edge_map.values(), lw = 1., colors = 'red')\n",
    "    line_list.set_zorder(0)\n",
    "    ax.add_collection(line_list)\n",
    "    \n",
    "    if true_edges != None:\n",
    "        line_list = LineCollection([(S[i], S[j]) for i, j in true_edges], lw = 15., colors = 'grey', alpha=0.4)\n",
    "        line_list.set_zorder(0)\n",
    "        ax.add_collection(line_list)\n",
    "    \n",
    "    ax.add_artist(plot.Circle((0.0, 0.0), 1.0, fill = False, lw = 1., color = 'black'))\n",
    "\n",
    "    # Job done\n",
    "    plot.title(title)\n",
    "    plot.show()\n",
    "    \n",
    "#--- Utils------------------------\n",
    "\n",
    "def hamming_distance(x, y):\n",
    "    return (x.astype(np.int32) ^ y.astype(np.int32)).sum()\n",
    "\n",
    "def distance_beltrami(p,q): #with p and q in euclidean coordinates and their norm<1\n",
    "    return np.arccosh((1-np.dot(p,q))/np.sqrt((1-np.linalg.norm(p)**2)*(1-np.linalg.norm(q)**2)))  \n",
    "    \n",
    "def build_tree_geodesic_distance_matrix(original_tree):    \n",
    "    tree_geodesic_distance_matrix = np.zeros((original_tree.shape[0], original_tree.shape[0]))   \n",
    "    for i in range(tree_geodesic_distance_matrix.shape[0]):\n",
    "        for j in range(tree_geodesic_distance_matrix.shape[1]):\n",
    "            tree_geodesic_distance_matrix[i][j] = hamming_distance(original_tree[i], original_tree[j]) \n",
    "    return tree_geodesic_distance_matrix\n",
    "        \n",
    "def build_latent_hyperbolic_distance_matrix(z_normalized):     \n",
    "    #latent_hyperbolic_distance_matrix = np.zeros((original_tree.shape[0], original_tree.shape[0]))  \n",
    "    latent_hyperbolic_distance_matrix = np.zeros((z_normalized.shape[0], z_normalized.shape[0]))      \n",
    "    for i in range(latent_hyperbolic_distance_matrix.shape[0]):\n",
    "        #print('i: ', i)\n",
    "        for j in range(latent_hyperbolic_distance_matrix.shape[1]):\n",
    "            #print('i: ', i,', j: ',j)\n",
    "            #print('j: ', j)\n",
    "            if i==j: #to avoid computing and getting nan, since we know that d(i,i)=0\n",
    "                latent_hyperbolic_distance_matrix[i][j] = 0.0\n",
    "            else:\n",
    "                latent_hyperbolic_distance_matrix[i][j] = distance_beltrami(z_normalized[i],z_normalized[j])\n",
    "    #print('done')\n",
    "    return latent_hyperbolic_distance_matrix\n",
    "\n",
    "def build_hyperbolically_weighted_adjacency_matrix(edges, z_normalized):     \n",
    "    hyperbolically_weighted_adjacency_matrix = np.zeros((z_normalized.shape[0], z_normalized.shape[0]))          \n",
    "    for edge in edges:\n",
    "        indices = []\n",
    "        for i in edge:\n",
    "            indices.append(i) \n",
    "        hyperbolically_weighted_adjacency_matrix[indices[0]][indices[1]] = distance_beltrami(z_normalized[indices[0]],z_normalized[indices[1]]) \n",
    "        hyperbolically_weighted_adjacency_matrix[indices[1]][indices[0]] = distance_beltrami(z_normalized[indices[1]],z_normalized[indices[0]]) \n",
    "\n",
    "    #print('done')\n",
    "    return hyperbolically_weighted_adjacency_matrix\n",
    "\n",
    "def build_binary_adjacency_matrix(edges, z_normalized):     \n",
    "    binary_adjacency_matrix = np.zeros((z_normalized.shape[0], z_normalized.shape[0]))          \n",
    "    for edge in edges:\n",
    "        indices = []\n",
    "        for i in edge:\n",
    "            indices.append(i) \n",
    "        binary_adjacency_matrix[indices[0]][indices[1]] = 1.0 \n",
    "        binary_adjacency_matrix[indices[1]][indices[0]] = 1.0\n",
    "\n",
    "    #print('done')\n",
    "    return binary_adjacency_matrix\n",
    "\n",
    "#groundtruth tree edges\n",
    "def build_true_edges(tree_geodesic_distance_matrix):\n",
    "    true_edges = set()\n",
    "    for i in range(tree_geodesic_distance_matrix.shape[0]):\n",
    "        for j in range(tree_geodesic_distance_matrix.shape[1]):\n",
    "            if tree_geodesic_distance_matrix[i][j]==1.:\n",
    "                temp=frozenset({i, j})\n",
    "                true_edges.add(temp) \n",
    "    return true_edges\n",
    "\n",
    "def build_edges(distance_matrix):\n",
    "    edges = set()\n",
    "    for i in range(distance_matrix.shape[0]):\n",
    "        for j in range(distance_matrix.shape[1]):\n",
    "            if distance_matrix[i][j]>0.:\n",
    "                temp=frozenset({i, j})\n",
    "                edges.add(temp) \n",
    "    return edges\n",
    "\n",
    "def convert_tri_list_to_edges(tri_list):\n",
    "    edges = set()\n",
    "    for l in tri_list:\n",
    "        edges.add( frozenset({l[0], l[1]}) )\n",
    "        edges.add( frozenset({l[1], l[2]}) )\n",
    "        edges.add( frozenset({l[2], l[0]}) )\n",
    "    return edges\n",
    "\n",
    "#############\n",
    "################\n",
    "#####################\n",
    "##############\n",
    "##########\n",
    "def get_edge_map(tri_list):    \n",
    "    edge_map = { }\n",
    "    for i, tri in enumerate(tri_list):\n",
    "        for edge in itertools.combinations(tri, 2):\n",
    "            #print(edge)\n",
    "            edge = tuple(sorted(edge))\n",
    "            if edge in edge_map:\n",
    "                edge_map[edge].append(i)\n",
    "            else:\n",
    "                edge_map[edge] = [i]  \n",
    "    return edge_map\n",
    "    \n",
    "def convert_tri_list_to_edges_real(tri_list, V, edge_map, voronoi_cell_map):\n",
    "    tri_list_real = []\n",
    "    V_real = []\n",
    "    edges_real = set()\n",
    "    number_of_delaunay_edges_corresponding_to_halflines = 0\n",
    "    for i,triangle in enumerate(tri_list):\n",
    "        if np.linalg.norm(V[i]) < 1:\n",
    "            edges_real.add( frozenset({triangle[0], triangle[1]}) )\n",
    "            edges_real.add( frozenset({triangle[1], triangle[2]}) )\n",
    "            edges_real.add( frozenset({triangle[2], triangle[0]}) )            \n",
    "        else:\n",
    "            triangle_edges = [tuple(sorted((triangle[0], triangle[1]))), tuple(sorted((triangle[1], triangle[2]))), tuple(sorted((triangle[2], triangle[0]))) ]\n",
    "            for triangle_edge in triangle_edges:\n",
    "                #print('triangle_edge:', triangle_edge)\n",
    "                associated_triangles = edge_map[tuple(triangle_edge)] # all the triangles from which triangle_edge is an edge, should be 2 triangles in general, but 1 triangle for some cases   \n",
    "                #print('associated_triangles:', associated_triangles)\n",
    "                if len(associated_triangles)==2:\n",
    "                    if min(np.linalg.norm(V[associated_triangles[0]]), np.linalg.norm(V[associated_triangles[1]]))<1:    \n",
    "                        #print('One Voronoi point is outside and the other inside the unit disk.')\n",
    "                        edges_real.add( frozenset({triangle_edge[0], triangle_edge[1]}) )\n",
    "                    else: #that is: the two associated voronoi points (end points of the voronoi segment which is the dual of the delaunay edge) are outside the unit disk, then if the line intersect the unit disk and the intersections belong to the voronoi segment then the corresponding delaunay edge should be kept\n",
    "                        #print('The two Voronoi points are outside the unit disk.')\n",
    "                        m = ( V[associated_triangles[1]][1]-V[associated_triangles[0]][1] ) / ( V[associated_triangles[1]][0]-V[associated_triangles[0]][0] ) #coeff directeur de la droite passant par les voronoi points                    \n",
    "                        p = V[associated_triangles[0]][1] - m*V[associated_triangles[0]][0] #ordonnée à l'origine\n",
    "                        delta = 4*(m**2-p**2+1)\n",
    "                        if delta>0: #that is if the line interects the cirlce at two places\n",
    "                            #print('The line passing through the two Voronoi points is intersecting the disk.')\n",
    "                            a = (1+m**2)\n",
    "                            b = 2*m*p\n",
    "                            c = p**2-1 \n",
    "                            sol1_x = (-b-np.sqrt(delta))/(2*a)\n",
    "                            sol2_x = (-b+np.sqrt(delta))/(2*a)\n",
    "                            sol1_y = m*sol1_x+p\n",
    "                            sol2_y = m*sol2_x+p\n",
    "                            min_voronoi_x = min(V[associated_triangles[0]][0], V[associated_triangles[1]][0])\n",
    "                            max_voronoi_x = max(V[associated_triangles[0]][0], V[associated_triangles[1]][0])       \n",
    "                            min_voronoi_y = min(V[associated_triangles[0]][1], V[associated_triangles[1]][1]) \n",
    "                            max_voronoi_y = max(V[associated_triangles[0]][1], V[associated_triangles[1]][1])\n",
    "                            if ( min_voronoi_x<=min(sol1_x,sol2_x) ) and ( max_voronoi_x>=max(sol1_x,sol2_x) ) and ( min_voronoi_y<=min(sol1_y,sol2_y) ) and ( max_voronoi_y>=max(sol1_y,sol2_y) ):\n",
    "                                #print('The two intersections points belong to the Voronoi segment.')\n",
    "                                edges_real.add( frozenset({triangle_edge[0], triangle_edge[1]}) )\n",
    "                            #edges_real.add( frozenset({triangle_edge[0], triangle_edge[1]}) )\n",
    "                        #else:\n",
    "                            #print('The line passing through the two Voronoi points is not intersecting the disk, or is tangent to it.')\n",
    "                elif len(associated_triangles)==1: #i.e. delaunay edge is the edge of only one triangle, i.e. the corresponding voronoi edge is a half-line starting from one voronoi point\n",
    "                    #print('This is a half-line')\n",
    "                    #if half-line intersects the unit circle then we keep it\n",
    "#                     print('triangle_edge:', triangle_edge)\n",
    "#                     print('i:', i)\n",
    "#                     print('V[i]:', V[i])\n",
    "#                     print('voronoi_cell_map[triangle_edge[0]]:', voronoi_cell_map[triangle_edge[0]])\n",
    "#                     print('voronoi_cell_map[triangle_edge[1]]:', voronoi_cell_map[triangle_edge[1]])\n",
    "                    for triangle_edge_index in [0,1]:\n",
    "                        for border in voronoi_cell_map[triangle_edge[triangle_edge_index]]:\n",
    "                            #print('border', border)\n",
    "                            if border[0]==(i, -1): #half-line border from the i-th Voronoi point to infinite\n",
    "#                                 print('The border (i, -1) is:', border)\n",
    "#                                 print('V[i]:', V[i])\n",
    "                                U = border[1][1] #unit vector direction of the half-line from Voronoi point to infinite\n",
    "                                radial = -border[1][0] #border[1][0] should be equal to array(V[i])  \n",
    "                                radial = radial/np.linalg.norm(radial) #unit vector direction from Voronoi point to origin of the Klein-Beltrami disk (center of the unit disk)\n",
    "                                theta = np.arccos(np.dot(U,radial))\n",
    "                                alpha = np.arcsin(1/np.linalg.norm(V[i])) #alpha is the limit angle for when the half-line is tangent to the unit circle\n",
    "#                                 print('U', U)\n",
    "#                                 print('radial', radial)\n",
    "#                                 print('theta', theta)\n",
    "#                                 print('alpha', alpha)\n",
    "                                if theta<alpha or theta>(2*np.pi - alpha): #i.e. the half-line intersects the unit circle\n",
    "                                    edges_real.add( frozenset({triangle_edge[0], triangle_edge[1]}) )\n",
    "                                    number_of_delaunay_edges_corresponding_to_halflines +=1\n",
    "#                                     print('The half-line intersects the unit circle.')\n",
    "#                                 else:\n",
    "#                                     print('The half-line does not intersect the unit circle.')                 \n",
    "#     print('Number of Delaunay edges corresponding to half-lines:', number_of_delaunay_edges_corresponding_to_halflines)                                    \n",
    "    return edges_real\n",
    "                \n",
    "#voronoi_cell_map[point data i.e. triangle_edge[0]][the border having (-1, i) or (i, -1)][1][1]    \n",
    "\n",
    "\n",
    "#                     m = U[1]/U[0]\n",
    "#                     p = V[associated_triangles[0]][1] - m*V[associated_triangles[0]][0] #ordonnée à l'origine\n",
    "#                     delta = 4*(m**2-p**2+1)\n",
    "#                     if delta>0: #that is if the line interects the cirlce at two places\n",
    "#                         #print('The line passing through the two Voronoi points is intersecting the disk.')\n",
    "#                         a = (1+m**2)\n",
    "#                         b = 2*m*p\n",
    "#                         c = p**2-1 \n",
    "#                         sol1_x = (-b-np.sqrt(delta))/(2*a)\n",
    "#                         sol2_x = (-b+np.sqrt(delta))/(2*a)\n",
    "#                         sol1_y = m*sol1_x+p\n",
    "#                         sol2_y = m*sol2_x+p \n",
    "\n",
    "\n",
    "################\n",
    "###########\n",
    "####\n",
    "##\n",
    "\n",
    "def sets_ratio(set1, set2):   \n",
    "    set1_in_set2 = 0\n",
    "    for edge in set1:\n",
    "        if {edge}.issubset(set2):\n",
    "            set1_in_set2+=1\n",
    "    print('Number of edges of set1 also in set2: ', set1_in_set2)\n",
    "    return set1_in_set2/len(set2)\n",
    "\n",
    "###def true_edges_ratio(edges, true_edges=true_edges):\n",
    "def true_edges_ratio(edges, true_edges):\n",
    "    #print('len(true_edges): ', len(true_edges))\n",
    "    #print('len(edges_filtered_closest): ', len(edges_filtered_closest))\n",
    "    intersection = true_edges.intersection(edges)\n",
    "    #print('len(intersection): ', len(intersection))\n",
    "    #print('len(intersection)/len(true_edges): ', len(intersection)/len(true_edges))\n",
    "    external_true_edges_ratio = len(intersection)/len(true_edges) #This gives the ratio of how many true edges of the graph are recovered w.r.t. the groundtruth true edges.\n",
    "    print('The external_true_edges_ratio=len(intersection)/len(true_edges) is:', external_true_edges_ratio)\n",
    "    internal_true_edges_ratio = len(intersection)/len(edges) #This gives the ratio of true edges in this graph among edges of this same graph.\n",
    "    print('The internal_true_edges_ratio=len(intersection)/len(edges) is:', internal_true_edges_ratio) \n",
    "    return [external_true_edges_ratio, internal_true_edges_ratio]\n",
    "\n",
    "def edges_hamming_score(edges): #Should be 1 if only true edges, i.e. edges which are also edges in the original tree\n",
    "    score = 0\n",
    "    for e in edges:\n",
    "        vertex1, vertex2 = e\n",
    "        score = score + hamming_distance(original_tree[vertex1], original_tree[vertex2])\n",
    "    score = score/len(edges)\n",
    "    print('The hamming score is:', score)\n",
    "    return score\n",
    "\n",
    "def edges_filtering_two_closest(edges):\n",
    "    edges_copy = edges.copy()\n",
    "    edges_filtered_closest = set()\n",
    "    #leafs = [0]\n",
    "    leafs = [0, 1]\n",
    "    explored_leafs = []\n",
    "    c = 0\n",
    "    while c<1:\n",
    "        #print('start', c)\n",
    "        #print('leafs:', leafs)\n",
    "        for vertex in leafs:\n",
    "            leafs.remove(vertex)\n",
    "            #print('current leaf:', vertex)\n",
    "            current_best_neighbours = []\n",
    "            seen_edges = []\n",
    "            for edge in edges_copy: \n",
    "                if vertex in edge:\n",
    "                    seen_edges.append(edge)\n",
    "                    for v in edge:\n",
    "                        if v!=vertex and v not in explored_leafs and v not in leafs:\n",
    "                            potential_neighbour = v \n",
    "                            #print('potential_neighbour:', potential_neighbour)\n",
    "                            if len(current_best_neighbours)<2:\n",
    "                                current_best_neighbours.append([potential_neighbour, distance_beltrami(z_normalized[vertex], z_normalized[potential_neighbour])])\n",
    "                            else:\n",
    "                                if current_best_neighbours[0][1]>current_best_neighbours[1][1]:\n",
    "                                    current_max_index, current_max_value = current_best_neighbours[0][0], current_best_neighbours[0][1]\n",
    "                                else:\n",
    "                                    current_max_index, current_max_value = current_best_neighbours[1][0], current_best_neighbours[1][1]                      \n",
    "                                dist = distance_beltrami(z_normalized[vertex], z_normalized[potential_neighbour])\n",
    "                                if dist < current_max_value:\n",
    "                                    current_best_neighbours.remove([current_max_index, current_max_value])\n",
    "                                    current_best_neighbours.append([potential_neighbour, dist])\n",
    "            #print('current_best_neighbours', current_best_neighbours)\n",
    "            for edge in seen_edges:\n",
    "                edges_copy.remove(edge)\n",
    "            if len(current_best_neighbours)>0:\n",
    "                if len(current_best_neighbours)==1:\n",
    "                    edges_filtered_closest.add(frozenset({vertex, current_best_neighbours[0][0]}))\n",
    "                    leafs.append(current_best_neighbours[0][0])\n",
    "                else:\n",
    "                    edges_filtered_closest.add(frozenset({vertex, current_best_neighbours[0][0]}))\n",
    "                    edges_filtered_closest.add(frozenset({vertex, current_best_neighbours[1][0]}))\n",
    "                    leafs.append(current_best_neighbours[0][0])\n",
    "                    leafs.append(current_best_neighbours[1][0])\n",
    "            #print('edges_filtered_closest:', edges_filtered_closest)\n",
    "            explored_leafs.append(vertex)\n",
    "        #print(len(leafs))\n",
    "        if len(leafs) == 0:\n",
    "            c +=1\n",
    "        #print('end', c)\n",
    "    return edges_filtered_closest\n",
    "\n",
    "def midpoint_hyperbolic_dichotomy(a, b, epsilon):\n",
    "    a_new = a.copy()\n",
    "    b_new = b.copy()\n",
    "    midpoint = (a_new+b_new)/2\n",
    "    err = distance_beltrami(a, midpoint)-distance_beltrami(b, midpoint)\n",
    "    while np.abs(err) > epsilon:\n",
    "        if err > 0:\n",
    "            a_new = a_new\n",
    "            b_new = midpoint\n",
    "            midpoint = (a_new+b_new)/2\n",
    "        else:\n",
    "            a_new = midpoint\n",
    "            b_new = b_new\n",
    "            midpoint = (a_new+b_new)/2\n",
    "        err = distance_beltrami(a, midpoint)-distance_beltrami(b, midpoint)\n",
    "    return midpoint\n",
    "\n",
    "def gabriel_from_delaunay(edges_delaunay, z_encodings, epsilon):\n",
    "    edges_gabriel = set()\n",
    "    for edge in edges_delaunay:\n",
    "        #print('edge', edge)\n",
    "        vertices = []\n",
    "        for vertex in edge:\n",
    "            vertices.append(vertex)\n",
    "        vertex1 = vertices[0]\n",
    "        vertex2 = vertices[1] \n",
    "        midpoint = midpoint_hyperbolic_dichotomy(z_encodings[vertex1], z_encodings[vertex2], epsilon) #Approximated hyperbolic midpoint        exit=0\n",
    "        exit=0\n",
    "        for i in range(z_encodings.shape[0]):\n",
    "            if i!=vertex1 and i!=vertex2:\n",
    "                if distance_beltrami(midpoint, z_encodings[i])< max(distance_beltrami(midpoint, z_encodings[vertex1]), distance_beltrami(midpoint, z_encodings[vertex2])):\n",
    "                    exit+=1\n",
    "                    #print('distance_beltrami(midpoint, z)', distance_beltrami(midpoint, z_encodings[i]))\n",
    "                    #print('max(dist_beltrami1, dist_beltrami1)', max(distance_beltrami(midpoint, z_encodings[vertex1]), distance_beltrami(midpoint, z_encodings[vertex2])))\n",
    "                    break\n",
    "        #print('exit', exit)\n",
    "        if exit==0:\n",
    "            edges_gabriel.add(edge)\n",
    "    return edges_gabriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd1f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hyperbolic_cdist(A,B): #Compute (Beltrami) hyperbolic distance between each pair of the two collections of inputs.\n",
    "    hyperbolic_cdist = np.zeros((A.shape[0], B.shape[0])) \n",
    "    for i in range(hyperbolic_cdist.shape[0]): \n",
    "        for j in range(hyperbolic_cdist.shape[1]):\n",
    "            hyperbolic_cdist[i][j] = distance_beltrami(A[i], B[j])\n",
    "            #print(hyperbolic_cdist[i][j])\n",
    "    return hyperbolic_cdist\n",
    "\n",
    "def hyperbolic_Wasserstein_dist(A,B, hyperbolic_cdist):\n",
    "    row_ind, col_ind = linear_sum_assignment(hyperbolic_cdist)\n",
    "    hyperbolic_Wasserstein_dist = hyperbolic_cdist[row_ind, col_ind].sum()       #mean and normalize to be between 0 and 1\n",
    "    return hyperbolic_Wasserstein_dist\n",
    "\n",
    "def hyperbolic_Chamfer_dist(A,B, hyperbolic_cdist): #symmetrized Chamfer distance\n",
    "    c_dist_square = hyperbolic_cdist ** 2\n",
    "\n",
    "    # Find the minimum squared distance along last axis\n",
    "    #Example for a NxP distance matrix, gives a vector of N elements: for each of the N rows of the NxP matrix, gives the minimal value between the P values. i.e. for each point of the first set, it gives the distance of the closest point from the other set. \n",
    "    min_squared_dist_a = np.min(c_dist_square, axis=-1)\n",
    "    # Take the mean\n",
    "    average_min_squared_dist_a = np.mean(min_squared_dist_a, axis=-1)    \n",
    "    \n",
    "    # Find the minimum squared distance along first axis\n",
    "    min_squared_dist_b = np.min(c_dist_square, axis=0)\n",
    "    # Take the mean along last axis\n",
    "    average_min_squared_dist_b = np.mean(min_squared_dist_b, axis=-1)\n",
    "    \n",
    "    return (average_min_squared_dist_a + average_min_squared_dist_b)/2\n",
    "\n",
    "def hyperbolic_Hausdorff_dist(A,B, hyperbolic_cdist):\n",
    "\n",
    "    c_dist_square = hyperbolic_cdist ** 2\n",
    "    \n",
    "    # Find the minimum distance along last axis\n",
    "    #Example for a NxP distance matrix, gives a vector of N elements: for each of the N rows of the NxP matrix, gives the minimal value between the P values. i.e. for each point of the first set, it gives the distance of the closest point from the other set. \n",
    "    dist_a = np.min(c_dist_square, axis=-1)\n",
    "    # Take the max\n",
    "    max_dist_a = np.max(dist_a, axis=-1)    \n",
    "    \n",
    "    # Find the minimum distance along first axis\n",
    "    dist_b = np.min(c_dist_square, axis=0)\n",
    "    # Take the max along last axis\n",
    "    max_dist_b = np.max(dist_b, axis=-1)\n",
    "    \n",
    "    return max(max_dist_a, max_dist_b)\n",
    "\n",
    "#####################################################3\n",
    "\n",
    "\n",
    "def distillation(delaunay, cluster_labels): #input delaunay is a set of frozensets, and cluster_labels an array with the cluster label for each point\n",
    "    \n",
    "    distilled_delaunay = {}\n",
    "    for label in sorted(set(cluster_labels)):\n",
    "        distilled_delaunay[label] = set()\n",
    "    crossed_edges = set()\n",
    "    \n",
    "    for edge in delaunay:\n",
    "        (v0,v1) = edge\n",
    "        if cluster_labels[v0]==cluster_labels[v1]:\n",
    "            distilled_delaunay[cluster_labels[v0]].add(edge)\n",
    "        else:\n",
    "            crossed_edges.add(edge)\n",
    "    \n",
    "    return {'distilled_delaunay': distilled_delaunay, 'crossed_edges': crossed_edges}\n",
    "\n",
    "#################################################\n",
    "\n",
    "\n",
    "def eval_scores_dga_local(distilled_delaunay, cluster_labels, R_indices, E_indices, get_edges_family = False):\n",
    "\n",
    "    if get_edges_family == True:\n",
    "        edges_family = {'intracluster': {'RR':set(), 'EE':set(), 'RE_U_ER':set()}, 'intercluster': {'RR':set(), 'EE':set(), 'RE_U_ER':set()}}\n",
    "    \n",
    "    scores_local = {}\n",
    "    for label in distilled_delaunay.keys():\n",
    "        scores_local[label] = {'cardRvertices':0, 'cardEvertices':0, 'cardRedges':0, 'cardEedges':0, 'card_edges':0, 'consistency':[], 'quality': []}\n",
    "\n",
    "    for R_index in R_indices:\n",
    "        scores_local[cluster_labels[R_index]]['cardRvertices']+=1\n",
    "    for E_index in E_indices:\n",
    "        scores_local[cluster_labels[E_index]]['cardEvertices']+=1\n",
    "\n",
    "    for label in distilled_delaunay.keys():\n",
    "        for (v0,v1) in distilled_delaunay[label]:\n",
    "            scores_local[label]['card_edges']+=1\n",
    "            if (v0 in E_indices) and (v1 in E_indices):\n",
    "                scores_local[label]['cardEedges']+=1\n",
    "                if get_edges_family == True: edges_family['intracluster']['EE'].add((v0,v1))\n",
    "            elif (v0 in R_indices) and (v1 in R_indices):\n",
    "                scores_local[label]['cardRedges']+=1\n",
    "                if get_edges_family == True: edges_family['intracluster']['RR'].add((v0,v1))\n",
    "            else:\n",
    "                if get_edges_family == True: edges_family['intracluster']['RE_U_ER'].add((v0,v1))\n",
    "                \n",
    "\n",
    "    for label in distilled_delaunay.keys():\n",
    "        scores_local[label]['consistency'] = 1 - abs(scores_local[label]['cardRvertices']-scores_local[label]['cardEvertices'])/(scores_local[label]['cardRvertices']+scores_local[label]['cardEvertices'])\n",
    "\n",
    "        if scores_local[label]['card_edges']>=1:\n",
    "            scores_local[label]['quality'] = 1 - (scores_local[label]['cardRedges']+scores_local[label]['cardEedges'])/scores_local[label]['card_edges']\n",
    "\n",
    "    if get_edges_family == True:\n",
    "        return scores_local, edges_family\n",
    "    else:\n",
    "        return scores_local\n",
    "    \n",
    "############################################\n",
    "\n",
    "\n",
    "def get_fundamental_components(scores_dga_local, eta_c, eta_q):\n",
    "    fundamental_components = []\n",
    "    for label in scores_dga_local.keys():\n",
    "        if scores_dga_local[label]['consistency']>eta_c and scores_dga_local[label]['quality']>eta_q:\n",
    "            fundamental_components.append(label)\n",
    "    return fundamental_components\n",
    "\n",
    "#######################################\n",
    "\n",
    "\n",
    "def eval_scores_dga_global(scores_dga_local, eta_c, eta_q, R_indices, E_indices, crossed_edges, delaunay, edges_family=None, get_edges_family=False):\n",
    "    \n",
    "    fundamental_components = get_fundamental_components(scores_dga_local, eta_c, eta_q)\n",
    "    \n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    for i in fundamental_components:\n",
    "        precision += scores_dga_local[i]['cardEvertices']\n",
    "        recall += scores_dga_local[i]['cardRvertices']\n",
    "    precision = precision / len(E_indices)\n",
    "    recall = recall / len(R_indices)\n",
    "    \n",
    "#     #DCA's original paper consistency global score according to their definition 3.3, but this would just give 1 if card(R)=card(E)...\n",
    "#     consistency = 1 - abs(len(R_indices)-len(E_indices))/(len(R_indices)+len(E_indices))\n",
    "    \n",
    "    #So we redefine the consistency global score as: weighted average of the consistency local scores. Weighted by the number of vertices in each cluster.\n",
    "    consistency = 0\n",
    "    total_number_considered_vertices = 0\n",
    "    for i in scores_dga_local.keys():\n",
    "        consistency += scores_dga_local[i]['consistency']*(scores_dga_local[i]['cardRvertices']+scores_dga_local[i]['cardEvertices'])\n",
    "        total_number_considered_vertices += scores_dga_local[i]['cardRvertices']+scores_dga_local[i]['cardEvertices']\n",
    "    consistency = consistency/total_number_considered_vertices\n",
    "    \n",
    "    card_crossed_edges_in_RR_or_EE = 0\n",
    "    if get_edges_family == False:\n",
    "        for (v0,v1) in crossed_edges:\n",
    "            if ( (v0 in E_indices) and (v1 in E_indices) ) or ( (v0 in R_indices) and (v1 in R_indices) ):\n",
    "                card_crossed_edges_in_RR_or_EE+=1\n",
    "    else:\n",
    "        for (v0,v1) in crossed_edges:\n",
    "            if v0 in E_indices:\n",
    "                if v1 in E_indices: #EE\n",
    "                    card_crossed_edges_in_RR_or_EE+=1\n",
    "                    edges_family['intercluster']['EE'].add((v0,v1))\n",
    "                else: #ER\n",
    "                    edges_family['intercluster']['RE_U_ER'].add((v0,v1))\n",
    "            elif v1 in R_indices: #RR\n",
    "                card_crossed_edges_in_RR_or_EE+=1\n",
    "                edges_family['intercluster']['RR'].add((v0,v1))\n",
    "            else: #RE\n",
    "                edges_family['intercluster']['RE_U_ER'].add((v0,v1))\n",
    "                \n",
    "    \n",
    "    card_noncrossed_edges_in_RR_or_EE = 0\n",
    "    for label in scores_dga_local.keys():\n",
    "        card_noncrossed_edges_in_RR_or_EE+= scores_dga_local[label]['cardRedges']+scores_dga_local[label]['cardEedges']\n",
    "    \n",
    "    card_edges_delaunay = len(delaunay)\n",
    "    if card_edges_delaunay < 1:\n",
    "        quality = 0\n",
    "    else:\n",
    "        quality = 1 - (card_noncrossed_edges_in_RR_or_EE+card_crossed_edges_in_RR_or_EE)/card_edges_delaunay\n",
    "        #Note: card_noncrossed_edges_in_RR_or_EE+card_crossed_edges_in_RR_or_EE = card_edges_in_RE_or_ER = number of heterogeneous edges\n",
    "    \n",
    "    \n",
    "    if get_edges_family == True:\n",
    "        return {'precision': precision, 'recall': recall, 'consistency': consistency, 'quality': quality}, edges_family\n",
    "    else:\n",
    "        return {'precision': precision, 'recall': recall, 'consistency': consistency, 'quality': quality}\n",
    "    \n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af0e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "#Load the data\n",
    "depth = 11\n",
    "original_tree = binary_tree.get_data(depth)\n",
    "\n",
    "#####START COMMENT#####\n",
    "train_hhh = binary_tree.get_data(depth)\n",
    "valid_hhh = train_hhh.copy()\n",
    "test_hhh = train_hhh.copy()\n",
    "dataset_randomness = 0.1\n",
    "\n",
    "train_hhh = binary_tree.ProbabilisticBinaryTreeDataset(train_hhh, eps=dataset_randomness)\n",
    "valid_hhh = binary_tree.ProbabilisticBinaryTreeDataset(valid_hhh, eps=dataset_randomness)\n",
    "#test_hhh = binary_tree.ProbabilisticBinaryTreeDataset(test_hhh, eps=dataset_randomness)\n",
    "\n",
    "\n",
    "#test_hhh_1 = binary_tree.ProbabilisticBinaryTreeDataset(test_hhh, eps=0.1)\n",
    "\n",
    "#test_hhh_2 = binary_tree.ProbabilisticBinaryTreeDataset(test_hhh, eps=0.2)\n",
    "#test_hhh_3 = binary_tree.ProbabilisticBinaryTreeDataset(test_hhh, eps=0.3)\n",
    "#test_hhh_4 = binary_tree.ProbabilisticBinaryTreeDataset(test_hhh, eps=0.4)\n",
    "#test_hhh_5 = binary_tree.ProbabilisticBinaryTreeDataset(test_hhh, eps=0.5)\n",
    "#test_hhh_6 = binary_tree.ProbabilisticBinaryTreeDataset(test_hhh, eps=0.6)\n",
    "#test_hhh_7 = binary_tree.ProbabilisticBinaryTreeDataset(test_hhh, eps=0.7)\n",
    "test_hhh_8 = binary_tree.ProbabilisticBinaryTreeDataset(test_hhh, eps=0.8)\n",
    "#test_hhh_9 = binary_tree.ProbabilisticBinaryTreeDataset(test_hhh, eps=0.9)\n",
    "#test_hhh_99 = binary_tree.ProbabilisticBinaryTreeDataset(test_hhh, eps=0.99)\n",
    "#test_hhh_10 = binary_tree.ProbabilisticBinaryTreeDataset(test_hhh, eps=1.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####END COMMENT#####\n",
    "\n",
    "#Do not forget to change the number here!\n",
    "#test_hhh = test_hhh_1\n",
    "#test_hhh = test_hhh_3\n",
    "test_hhh = test_hhh_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe593611",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './exp_500000iterations_11depth/depth11_avg_elbo_loss_snapshot_iter_100000/'\n",
    "\n",
    "#Load the model\n",
    "enc_linears_0_b = np.load(path+'encoder/head/linears/0/b.npy') \n",
    "enc_linears_0_W = np.load(path+'encoder/head/linears/0/W.npy') \n",
    "enc_output_b = np.load(path+'encoder/head/output/b.npy') \n",
    "enc_output_W = np.load(path+'encoder/head/output/W.npy') \n",
    "\n",
    "# dec_linears_0_b = np.load(path+'decoder/head/linears/0/b.npy') \n",
    "# dec_linears_0_W = np.load(path+'decoder/head/linears/0/W.npy') \n",
    "# dec_output_b = np.load(path+'decoder/head/output/b.npy') \n",
    "# dec_output_W = np.load(path+'decoder/head/output/W.npy') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acbaba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the latent representation of train and test concatenated\n",
    "z_hhh=[]\n",
    "L_hhh=[]\n",
    "train_hhh = np.array(train_hhh)\n",
    "for i in range(len(train_hhh)):\n",
    "    output_hhh = np.dot(enc_output_W, np.dot(enc_linears_0_W, train_hhh[i])+enc_linears_0_b) + enc_output_b  \n",
    "    means_hhh = [output_hhh[0], output_hhh[1]]\n",
    "    z_hhh.append(means_hhh)\n",
    "    #L_hhh.append(0)\n",
    "    L_hhh.append('blue')\n",
    "for i in range(len(test_hhh)):\n",
    "    output_hhh = np.dot(enc_output_W, np.dot(enc_linears_0_W, test_hhh[i])+enc_linears_0_b) + enc_output_b  \n",
    "    means_hhh = [output_hhh[0], output_hhh[1]]\n",
    "    z_hhh.append(means_hhh)\n",
    "    #L_hhh.append(1)\n",
    "    L_hhh.append('orange')\n",
    "\n",
    "    \n",
    "#need to normalize to get norms<1 in order to build the Hyperbolic Voronoi Diagram\n",
    "z_norms=[]\n",
    "pert = 0.001 #small perturbation to not have the maximal norm being normalized to exactly 1 because all the normalized norms should be stricly inferior to 1   \n",
    "for v in z_hhh:\n",
    "    z_norms.append(np.linalg.norm(v))\n",
    "max_z_norms = max(z_norms)+pert \n",
    "\n",
    "z_normalized=[]\n",
    "for i in range(len(z_hhh)):\n",
    "        z_normalized.append([z_hhh[i][0]/max_z_norms, z_hhh[i][1]/max_z_norms])\n",
    "\n",
    "#Convert from Poincaré to Beltrami-Cayley-Klein model latent representation\n",
    "z_beltrami = []\n",
    "for i in range(len(z_normalized)):\n",
    "    norm_squared = z_normalized[i][0]**2+z_normalized[i][1]**2 \n",
    "    z_beltrami.append([2*z_normalized[i][0]/(1+norm_squared), 2*z_normalized[i][1]/(1+norm_squared)])\n",
    "\n",
    "z_normalized=np.array(z_beltrami)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3096cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1edb610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize = (10, 10))\n",
    "# #plt.scatter(z_normalized[:,0], z_normalized[:,1], s=10, c=L_hhh, cmap='Spectral')\n",
    "# plt.scatter(np.array(z_normalized)[:,0], np.array(z_normalized)[:,1], s=20, c=L_hhh, cmap='Spectral')\n",
    "# #plt.title('Encodings of Train vs Perturbed test data (eps=0.1), after normalization in Klein, iter=100000', fontsize=12)\n",
    "\n",
    "# plt.axis('off')\n",
    "# plt.tight_layout(pad=0)\n",
    "\n",
    "# plt.savefig('ECML_Exp1_iter100000_Klein_eps0_1.png')\n",
    "# plt.savefig('ECML_Exp1_iter100000_Klein_eps0_1.pdf')\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6fdd762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize = (10, 10))\n",
    "# #plt.scatter(z_normalized[:,0], z_normalized[:,1], s=10, c=L_hhh, cmap='Spectral')\n",
    "# plt.scatter(np.array(z_normalized)[:,0], np.array(z_normalized)[:,1], s=20, c=L_hhh, cmap='Spectral')\n",
    "# #plt.title('Encodings of Train vs Perturbed test data (eps=0.3), after normalization in Klein, iter=100000', fontsize=12)\n",
    "\n",
    "# plt.axis('off')\n",
    "# plt.tight_layout(pad=0)\n",
    "\n",
    "# plt.savefig('ECML_Exp1_iter100000_Klein_eps0_3.png')\n",
    "# plt.savefig('ECML_Exp1_iter100000_Klein_eps0_3.pdf')\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed8d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (10, 10))\n",
    "#plt.scatter(z_normalized[:,0], z_normalized[:,1], s=10, c=L_hhh, cmap='Spectral')\n",
    "plt.scatter(np.array(z_normalized)[:,0], np.array(z_normalized)[:,1], s=20, c=L_hhh, cmap='Spectral')\n",
    "#plt.title('Encodings of Train vs Perturbed test data (eps=0.8), after normalization in Klein, iter=100000', fontsize=12)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "plt.savefig('ECML_Exp1_iter100000_Klein_eps0_8.png')\n",
    "plt.savefig('ECML_Exp1_iter100000_Klein_eps0_8.pdf')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d96743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039a9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_beltrami\n",
    "\n",
    "percentage = 50/100\n",
    "threshold = int(percentage * z_normalized.shape[0])\n",
    "#print(threshold)\n",
    "\n",
    "R_indices = [i for i in range(threshold)]\n",
    "E_indices=[i for i in range(threshold, z_normalized.shape[0])]\n",
    "#Be careful, the Delaunay built from z_normalized should be such that z_normalized=(R_indices U E_indices)  \n",
    "    \n",
    "    \n",
    "##############  Hyperbolic Chamfer and Wasserstein ##############\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "threshold = int(len(z_hhh_class1))\n",
    "A = z_normalized[:threshold]\n",
    "B = z_normalized[threshold:]\n",
    "    \n",
    "print('Computing the Hyperbolic Chamfer distance...')\n",
    "hyperbolic_cdistance = hyperbolic_cdist(A,B)\n",
    "end_time_cdist = time.time()\n",
    "\n",
    "hyperbolic_Chamfer_distance = hyperbolic_Chamfer_dist(A,B, hyperbolic_cdistance)\n",
    "end_time_chamfer = time.time()\n",
    "\n",
    "hyperbolic_Wasserstein_distance = hyperbolic_Wasserstein_dist(A,B, hyperbolic_cdistance) \n",
    "end_time_wasserstein = time.time()\n",
    "\n",
    "elapsed_time_chamfer = end_time_chamfer - start_time\n",
    "print(\"Elapsed time for Hyperbolic Chamfer: \", elapsed_time_chamfer) \n",
    "print('Hyperbolic Chamfer distance: ', hyperbolic_Chamfer_distance)\n",
    "\n",
    "elapsed_time_wasserstein = end_time_wasserstein - end_time_chamfer + end_time_cdist-start_time\n",
    "print(\"Elapsed time for Hyperbolic Wasserstein: \", elapsed_time_wasserstein) \n",
    "#print('Hyperbolic Wasserstein distance: ', hyperbolic_Wasserstein_distance)\n",
    "print('Hyperbolic Wasserstein distance pseudo-normalized: ', hyperbolic_Wasserstein_distance/len(z_beltrami))\n",
    "\n",
    "\n",
    "####################\n",
    "    \n",
    "# Start timer\n",
    "start_time = time.time()    \n",
    "    \n",
    "#Compute the hyperbolic (or Euclidean) Voronoi and Delaunay Complex\n",
    "print('Computing the Hyperbolic Delaunay...')\n",
    "    \n",
    "S = np.array(z_normalized)\n",
    "\n",
    "centers = np.zeros(S.shape)\n",
    "radii = np.zeros(S.shape[0])\n",
    "\n",
    "for i in range(S.shape[0]):\n",
    "    ###To get hyperbolic Delaunay:\n",
    "    centers[i] = S[i]/(2*np.sqrt(1-(np.linalg.norm(S[i]))**2))\n",
    "    radii[i] = (np.linalg.norm(S[i]))**2/(4*(1-(np.linalg.norm(S[i]))**2))-1/np.sqrt(1-(np.linalg.norm(S[i]))**2)   \n",
    "\n",
    "    ###To get Euclidean Delaunay:\n",
    "    #centers[i] = S[i] \n",
    "\n",
    "\n",
    "\n",
    "# Compute the power triangulation of the circles\n",
    "tri_list, V = get_power_triangulation(centers, radii)\n",
    "\n",
    "# Compute the Voronoi cells\n",
    "voronoi_cell_map = get_voronoi_cells(centers, V, tri_list)\n",
    "\n",
    "edge_map = get_edge_map(tri_list) ##\n",
    "#edges = convert_tri_list_to_edges(tri_list)\n",
    "edges_real = convert_tri_list_to_edges_real(tri_list, V, edge_map, voronoi_cell_map)\n",
    "    \n",
    "################ Optional \"Ddistillation\" ################\n",
    "\n",
    "#     # Hyperbolic HDBSCAN\n",
    "#     latent_hyperbolic_distance_matrix = build_latent_hyperbolic_distance_matrix(z_normalized) #distance matrix of the encodings in the hyperbolic latent space w.r.t.hyperbolic distance\n",
    "#     distance_matrix = latent_hyperbolic_distance_matrix\n",
    "\n",
    "#     ### allow_single_cluster = False or True    \n",
    "\n",
    "#     #clusterer = hdbscan.HDBSCAN(metric='precomputed') \n",
    "#     #Or: #by default cluster_selection_method=\"eom\" but try cluster_selection_method=\"leaf\"\n",
    "#     #clusterer = hdbscan.HDBSCAN(cluster_selection_epsilon=0.0, algorithm='best', alpha=1.0, max_cluster_size=0, approx_min_span_tree=True, gen_min_span_tree=True, leaf_size=40, metric='precomputed', min_cluster_size=40, min_samples=None, p=None, cluster_selection_method='eom', allow_single_cluster=True, match_reference_implementation=False)      \n",
    "#     #clusterer = hdbscan.HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True, gen_min_span_tree=True, leaf_size=40, metric='precomputed', min_cluster_size=40, min_samples=None, p=None, cluster_selection_method='leaf')   \n",
    "#     clusterer = hdbscan.HDBSCAN(cluster_selection_epsilon=0.0, algorithm='best', alpha=1.0, max_cluster_size=0, approx_min_span_tree=True, gen_min_span_tree=True, leaf_size=40, metric='precomputed', min_cluster_size=5, min_samples=None, p=None, cluster_selection_method='eom', allow_single_cluster=True, match_reference_implementation=False)      \n",
    "#     clusterer.fit(distance_matrix)\n",
    "#     clusterer.labels_    \n",
    "    \n",
    "#     print('clusterer.labels_.max()', clusterer.labels_.max())\n",
    "#     print('Number of unclustered points: ', list(clusterer.labels_).count(-1))\n",
    "\n",
    "#     clusterer_labels = clusterer.labels_\n",
    "\n",
    "#     #Delaunay distillation\n",
    "#     print('Working on the Delaunay distillation...')\n",
    "######################\n",
    "\n",
    "# If we are just interested in global scores then don't need to do the clustering so we can skip HDBSCAN and do like if we have just one cluster\n",
    "    \n",
    "clusterer_labels = [0]*len(z_beltrami)\n",
    "    \n",
    "distillation_delaunay = distillation(edges_real, clusterer_labels)\n",
    "distilled_delaunay = distillation_delaunay['distilled_delaunay']\n",
    "crossed_edges = distillation_delaunay['crossed_edges']\n",
    "    \n",
    "#######################\n",
    "\n",
    "\n",
    "print('Computing HyperDGA scores')\n",
    "scores_dga_local, edges_family = eval_scores_dga_local(distilled_delaunay, clusterer_labels, R_indices, E_indices, get_edges_family = True)\n",
    "########################\n",
    "    \n",
    "scores_dga_global, edges_family = eval_scores_dga_global(scores_dga_local, 0.5, 0.3, R_indices, E_indices, crossed_edges, edges_real, edges_family, get_edges_family=True)\n",
    "    \n",
    "# End timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time for HyperDGA: \", elapsed_time)    \n",
    "    \n",
    "print('scores_dga_global: ', scores_dga_global)\n",
    "    \n",
    "#return scores_dga_global['quality'], hyperbolic_Chamfer_distance\n",
    "print('HyperDGA global quality score: ', scores_dga_global['quality'])\n",
    "print('HyperDGA distance: ', 1-scores_dga_global['quality'])\n",
    "# print('Hyperbolic Chamfer: ', hyperbolic_Chamfer_distance)\n",
    "# print('hyperbolic_Wasserstein_distance pseudo-normalized: ', hyperbolic_Wasserstein_distance/len(z_beltrami))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3f2a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(S, R_indices, E_indices, voronoi_cell_map, edges_family, margen=0, title='HyperDGA visualization'): \n",
    "    # Setup\n",
    "    #fig, ax = plot.subplots()\n",
    "    fig, ax = plot.subplots(figsize = (20, 20)) #20 #10 #100\n",
    "    plot.axis('equal')\n",
    "    plot.axis('off')\n",
    "    point_size=10#0.5\n",
    "#     for i in R_indices[1:]:\n",
    "#         plot.scatter(S[i, 0], S[i, 1], s=point_size, c='blue')\n",
    "#     for i in E_indices[1:]:\n",
    "#         plot.scatter(S[i, 0], S[i, 1], s=point_size, c='orange')\n",
    "#     plot.scatter(S[R_indices[0], 0], S[R_indices[0], 1], s=point_size, c='blue', label='R')\n",
    "#     plot.scatter(S[E_indices[0], 0], S[E_indices[0], 1], s=point_size, c='orange', label='E')\n",
    "\n",
    "#     #eps 0.1\n",
    "#     # Set min/max display size, as Matplotlib does it wrong\n",
    "#     min_corner_x = numpy.amin(S, axis = 0)-0.1 #- numpy.max(R)\n",
    "#     max_corner_x = numpy.amax(S, axis = 0)+0.125 #+ numpy.max(R)\n",
    "#     min_corner_y = numpy.amin(S, axis = 1)+0.05#-0.1 #- numpy.max(R)\n",
    "#     max_corner_y = numpy.amax(S, axis = 1)+0.625 #+ numpy.max(R)\n",
    "#     plot.xlim((min_corner_x[0], max_corner_x[0]))\n",
    "#     plot.ylim((min_corner_y[1], max_corner_y[1]))\n",
    "\n",
    "\n",
    "#     #eps 0.3\n",
    "#     # Set min/max display size, as Matplotlib does it wrong\n",
    "#     min_corner_x = numpy.amin(S, axis = 0)-0.1 #- numpy.max(R)\n",
    "#     max_corner_x = numpy.amax(S, axis = 0)+0.125 #+ numpy.max(R)\n",
    "#     min_corner_y = numpy.amin(S, axis = 1)+0.1#-0.1 #- numpy.max(R)\n",
    "#     max_corner_y = numpy.amax(S, axis = 1)+0.5 #+ numpy.max(R)\n",
    "#     plot.xlim((min_corner_x[0], max_corner_x[0]))\n",
    "#     plot.ylim((min_corner_y[1], max_corner_y[1]))\n",
    "\n",
    "    #eps 0.8\n",
    "    # Set min/max display size, as Matplotlib does it wrong\n",
    "    min_corner_x = numpy.amin(S, axis = 0)-0.1 #- numpy.max(R)\n",
    "    max_corner_x = numpy.amax(S, axis = 0)+0.125 #+ numpy.max(R)\n",
    "    min_corner_y = numpy.amin(S, axis = 1)+0.5#-0.1 #- numpy.max(R)\n",
    "    max_corner_y = numpy.amax(S, axis = 1)-0.1 #+ numpy.max(R)\n",
    "    plot.xlim((min_corner_x[0], max_corner_x[0]))\n",
    "    plot.ylim((min_corner_y[1], max_corner_y[1]))\n",
    "\n",
    "\n",
    "\n",
    "    # Plot the Voronoi cells\n",
    "    edge_map = { }\n",
    "    for segment_list in voronoi_cell_map.values():\n",
    "        for edge, (A, U, tmin, tmax) in segment_list:\n",
    "            edge = tuple(sorted(edge))\n",
    "            if edge not in edge_map:\n",
    "                if tmax is None:\n",
    "                    tmax = 10\n",
    "                if tmin is None:\n",
    "                    tmin = -10\n",
    "\n",
    "                edge_map[edge] = (A + tmin * U, A + tmax * U)\n",
    "\n",
    "#     line_list = LineCollection(edge_map.values(), lw = 1.0, colors = 'grey', linestyle='dashed', label='Voronoi cells')\n",
    "#     line_list.set_zorder(0)\n",
    "#     ax.add_collection(line_list)\n",
    "    \n",
    "#     if true_edges != None:\n",
    "#         line_list = LineCollection([(S[i], S[j]) for i, j in true_edges], lw = 15., colors = 'grey', alpha=0.4)\n",
    "#         line_list.set_zorder(0)\n",
    "#         ax.add_collection(line_list)\n",
    "\n",
    "    lw=1.0\n",
    "    alpha_transparent_edges = 0.3 #0.25\n",
    "    edges_intra_RR = LineCollection([(S[i], S[j]) for i, j in edges_family['intracluster']['RR']], lw = lw, colors = 'blue', alpha=1.0, label='intracluster RxR edge')\n",
    "    edges_intra_RR.set_zorder(0)\n",
    "    ax.add_collection(edges_intra_RR)\n",
    "\n",
    "    edges_intra_EE = LineCollection([(S[i], S[j]) for i, j in edges_family['intracluster']['EE']], lw = lw, colors = 'orange', alpha=1.0, label='intracluster ExE edge')\n",
    "    edges_intra_EE.set_zorder(0)\n",
    "    ax.add_collection(edges_intra_EE)\n",
    "\n",
    "    edges_intra_RE_U_ER = LineCollection([(S[i], S[j]) for i, j in edges_family['intracluster']['RE_U_ER']], lw = lw, colors = 'green', alpha=1.0, label='intracluster (RxE U ExR) edge')   \n",
    "    edges_intra_RE_U_ER.set_zorder(0)\n",
    "    ax.add_collection(edges_intra_RE_U_ER)\n",
    "\n",
    "    edges_inter_RR = LineCollection([(S[i], S[j]) for i, j in edges_family['intercluster']['RR']], lw = lw, colors = 'blue', alpha=alpha_transparent_edges, label='intercluster RxR edge')\n",
    "    edges_inter_RR.set_zorder(0)\n",
    "    ax.add_collection(edges_inter_RR)\n",
    "\n",
    "    edges_inter_EE = LineCollection([(S[i], S[j]) for i, j in edges_family['intercluster']['EE']], lw = lw, colors = 'orange', alpha=alpha_transparent_edges, label='intercluster ExE edge')\n",
    "    edges_inter_EE.set_zorder(0)\n",
    "    ax.add_collection(edges_inter_EE)\n",
    "\n",
    "    edges_inter_RE_U_ER = LineCollection([(S[i], S[j]) for i, j in edges_family['intercluster']['RE_U_ER']], lw = lw, colors = 'green', alpha=alpha_transparent_edges, label='intercluster (RxE U ExR) edge')\n",
    "    edges_inter_RE_U_ER.set_zorder(0)\n",
    "    ax.add_collection(edges_inter_RE_U_ER)\n",
    "    \n",
    "#    ax.add_artist(plot.Circle((0.0, 0.0), 1.0, fill = False, lw = 1., color = 'black'))\n",
    "\n",
    "    # Job done\n",
    "    #plot.title(title)\n",
    "    #leg=ax.legend(loc='lower right')\n",
    "    \n",
    "    plt.tight_layout(pad=0)\n",
    "\n",
    "    #plot.savefig(title+'.pdf')\n",
    "    #plot.savefig(title+'.png')\n",
    "    plot.savefig('ECML_exp1_HyperDGA_eps0_8_new.pdf')\n",
    "    plot.savefig('ECML_exp1_HyperDGA_eps0_8_new.png')\n",
    "\n",
    "#    plot.savefig('ECML_exp1_HyperDGA_eps0_8_dpi200.png', dpi=200) #try 200,3000, the least such that it looks good\n",
    "\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016d4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #epsilon = 0.1\n",
    "# visualize(S, R_indices, E_indices, voronoi_cell_map, edges_family, margen=0.0, title='HyperDGA visualization in Klein') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde075cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #epsilon = 0.3\n",
    "# visualize(S, R_indices, E_indices, voronoi_cell_map, edges_family, margen=0.0, title='HyperDGA visualization in Klein') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "436be621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon = 0.8\n",
    "visualize(S, R_indices, E_indices, voronoi_cell_map, edges_family, margen=0.0, title='HyperDGA visualization in Klein') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
